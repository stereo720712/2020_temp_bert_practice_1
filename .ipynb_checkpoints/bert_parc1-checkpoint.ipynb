{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./bert-master')\n",
    "import numpy as np # linear algebra\n",
    "import re, os\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "# BERT\n",
    "import optimization\n",
    "import run_classifier\n",
    "import tokenization\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8091840178340672589\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3141979340\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12901050375101259667\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file details\n",
    "directory = []\n",
    "file = []\n",
    "title = []\n",
    "text = []\n",
    "label = []\n",
    "datapath = './bbc-fulltext (document classification)/bbc/' \n",
    "for dirname, _ , filenames in os.walk(datapath):\n",
    "    #print('Directory: ', dirname)\n",
    "    #print('Subdir: ', dirname.split('/')[-1])\n",
    "    # remove the Readme.txt file\n",
    "    # will not find file in the second iteration so we skip the error\n",
    "    try:\n",
    "        filenames.remove('README.TXT')\n",
    "    except:\n",
    "        pass\n",
    "    for filename in filenames:\n",
    "        directory.append(dirname)\n",
    "        file.append(filename)\n",
    "        label.append(dirname.split('/')[-1])\n",
    "        fullpathfile = os.path.join(dirname,filename)\n",
    "        with open(fullpathfile, 'r', encoding=\"utf8\", errors='ignore') as infile:\n",
    "            intext = ''\n",
    "            firstline = True\n",
    "            for line in infile:\n",
    "                if firstline:\n",
    "                    title.append(line.replace('\\n',''))\n",
    "                    firstline = False\n",
    "                else:\n",
    "                    intext = intext + ' ' + line.replace('\\n','')\n",
    "            text.append(intext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quarterly profits at US media giant TimeWarn...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The dollar has hit its highest level against...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The owners of embattled Russian oil giant Yu...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>British Airways has blamed high fuel prices ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shares in UK drinks and food firm Allied Dom...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0    Quarterly profits at US media giant TimeWarn...  business\n",
       "1    The dollar has hit its highest level against...  business\n",
       "2    The owners of embattled Russian oil giant Yu...  business\n",
       "3    British Airways has blamed high fuel prices ...  business\n",
       "4    Shares in UK drinks and food firm Allied Dom...  business"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_COLUMN = 'text'\n",
    "LABEL_COLUMN = 'label'\n",
    "\n",
    "fulldf = pd.DataFrame(list(zip(directory, file, title, text, label)), \n",
    "               columns =['directory', 'file', 'title', 'text', 'label'])\n",
    "\n",
    "df = fulldf.filter(['text','label'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAE2CAYAAACaxNI3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXAElEQVR4nO3df7RdZX3n8feHAFoVFJYXzCJooitawd9mEKUzteIoVRTLiI2jDmukw7TSGR07Vej88EeblrrGrqlOmZZp66T+YtKqJeKqlUlFB5cVgygKyCIjFjJkSPAXjLZQwnf+2DvmJLk/TpJ7s0+e836tddc5+zn73PPNWed+8pxnP/vZqSokSW05YugCJEmLz3CXpAYZ7pLUIMNdkhpkuEtSg44cugCAxz72sbVy5cqhy5Ckw8r1119/T1XNzPbYRIT7ypUr2bx589BlSNJhJcnfzPWYwzKS1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSgiThDdTGsvPhTQ5cAwLcvffnQJfheSBqv557k20m+nuSrSTb3bccnuTrJbf3tcSP7X5JkS5Jbk7x0qYqXJM1uf4ZlfqaqnlVVa/rti4FNVbUa2NRvk+QUYC1wKnAWcFmSZYtYsyRpAQcz5n4OsL6/vx541Uj7FVV1f1XdDmwBTjuI15Ek7adxw72AzyS5PsmFfduJVbUNoL89oW8/Cbhz5Llb+7Y9JLkwyeYkm3fs2HFg1UuSZjXuAdUzququJCcAVyf55jz7Zpa22qeh6nLgcoA1a9bs87gk6cCN1XOvqrv62+3AJ+iGWe5Oshygv93e774VOHnk6SuAuxarYEnSwhYM9ySPTHLMrvvAS4BvABuB8/vdzgeu7O9vBNYmeViSVcBq4LrFLlySNLdxhmVOBD6RZNf+H6mqTyf5MrAhyQXAHcB5AFV1U5INwM3Ag8BFVbVzSaqXJM1qwXCvqm8Bz5yl/TvAmXM8Zx2w7qCrkyQdEJcfkKQGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoOOHLoAaSmtvPhTQ5fAty99+dAlaArZc5ekBhnuktQgw12SGjR2uCdZluSGJFf128cnuTrJbf3tcSP7XpJkS5Jbk7x0KQqXJM1tf3rubwZuGdm+GNhUVauBTf02SU4B1gKnAmcBlyVZtjjlSpLGMdZsmSQrgJcD64C39s3nAC/s768HrgHe3rdfUVX3A7cn2QKcBnxx0aqWtN+cOTRdxu25/2fgbcBDI20nVtU2gP72hL79JODOkf229m17SHJhks1JNu/YsWN/65YkzWPBcE9yNrC9qq4f83dmlrbap6Hq8qpaU1VrZmZmxvzVkqRxjDMscwbwyiQvAx4OHJvkQ8DdSZZX1bYky4Ht/f5bgZNHnr8CuGsxi5YkzW/BnntVXVJVK6pqJd2B0r+qqtcDG4Hz+93OB67s728E1iZ5WJJVwGrgukWvXJI0p4NZfuBSYEOSC4A7gPMAquqmJBuAm4EHgYuqaudBVypJGtt+hXtVXUM3K4aq+g5w5hz7raObWSNJE2caZg55hqokNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ1aMNyTPDzJdUm+luSmJO/q249PcnWS2/rb40aec0mSLUluTfLSpfwHSJL2NU7P/X7gRVX1TOBZwFlJTgcuBjZV1WpgU79NklOAtcCpwFnAZUmWLUHtkqQ5LBju1fl//eZR/U8B5wDr+/b1wKv6++cAV1TV/VV1O7AFOG0xi5YkzW+sMfcky5J8FdgOXF1VXwJOrKptAP3tCf3uJwF3jjx9a9+29++8MMnmJJt37NhxEP8ESdLexgr3qtpZVc8CVgCnJXnaPLtntl8xy++8vKrWVNWamZmZsYqVJI1nv2bLVNX3gWvoxtLvTrIcoL/d3u+2FTh55GkrgLsOtlBJ0vjGmS0zk+Qx/f2fAF4MfBPYCJzf73Y+cGV/fyOwNsnDkqwCVgPXLXLdkqR5HDnGPsuB9f2MlyOADVV1VZIvAhuSXADcAZwHUFU3JdkA3Aw8CFxUVTuXpnxJ0mwWDPequhF49izt3wHOnOM564B1B12dJOmAeIaqJDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUELhnuSk5N8NsktSW5K8ua+/fgkVye5rb89buQ5lyTZkuTWJC9dyn+AJGlf4/TcHwR+paqeCpwOXJTkFOBiYFNVrQY29dv0j60FTgXOAi5LsmwpipckzW7BcK+qbVX1lf7+fcAtwEnAOcD6frf1wKv6++cAV1TV/VV1O7AFOG2R65YkzWO/xtyTrASeDXwJOLGqtkH3HwBwQr/bScCdI0/b2rft/bsuTLI5yeYdO3YcQOmSpLmMHe5JHgV8DHhLVd07366ztNU+DVWXV9WaqlozMzMzbhmSpDGMFe5JjqIL9g9X1cf75ruTLO8fXw5s79u3AiePPH0FcNfilCtJGsc4s2UC/BFwS1X9zshDG4Hz+/vnA1eOtK9N8rAkq4DVwHWLV7IkaSFHjrHPGcAbgK8n+Wrf9mvApcCGJBcAdwDnAVTVTUk2ADfTzbS5qKp2LnbhkqS5LRjuVXUts4+jA5w5x3PWAesOoi5J0kHwDFVJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhq0YLgn+eMk25N8Y6Tt+CRXJ7mtvz1u5LFLkmxJcmuSly5V4ZKkuY3Tc//vwFl7tV0MbKqq1cCmfpskpwBrgVP751yWZNmiVStJGsuC4V5Vnwe+u1fzOcD6/v564FUj7VdU1f1VdTuwBThtcUqVJI3rQMfcT6yqbQD97Ql9+0nAnSP7be3b9pHkwiSbk2zesWPHAZYhSZrNYh9QzSxtNduOVXV5Va2pqjUzMzOLXIYkTbcDDfe7kywH6G+39+1bgZNH9lsB3HXg5UmSDsSBhvtG4Pz+/vnAlSPta5M8LMkqYDVw3cGVKEnaX0cutEOSjwIvBB6bZCvwDuBSYEOSC4A7gPMAquqmJBuAm4EHgYuqaucS1S5JmsOC4V5Vr53joTPn2H8dsO5gipIkHRzPUJWkBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1aMnCPclZSW5NsiXJxUv1OpKkfS1JuCdZBvwe8LPAKcBrk5yyFK8lSdrXUvXcTwO2VNW3quoB4ArgnCV6LUnSXlJVi/9Lk1cDZ1XVL/TbbwCeV1W/PLLPhcCF/eZTgFsXvZD991jgnqGLmBC+F7v5Xuzme7HbJLwXT6iqmdkeOHKJXjCztO3xv0hVXQ5cvkSvf0CSbK6qNUPXMQl8L3bzvdjN92K3SX8vlmpYZitw8sj2CuCuJXotSdJelircvwysTrIqydHAWmDjEr2WJGkvSzIsU1UPJvll4C+BZcAfV9VNS/Fai2yihokG5nuxm+/Fbr4Xu030e7EkB1QlScPyDFVJapDhLkkNMtwlqUFTHe5JzhinbRolOS7JM4auQ9KBmepwB94/ZttUSHJNkmOTHA98DfhAkt8Zuq4hJHlP/14clWRTknuSvH7oujScJL89TtukmMpwT/L8JL8CzCR568jPO+mmbk6rR1fVvcC5wAeq6rnAiweuaSgv6d+Ls+lOynsy8KvDlnToJTk3yW1JfpDk3iT3Jbl36LoG8o9nafvZQ17FmJZq+YFJdzTwKLp//zEj7fcCrx6koslwZJLlwGuAfzd0MQM7qr99GfDRqvpuMtuqGs17D/CKqrpl6EKGkuSXgDcBT0xy48hDxwBfGKaqhU1luFfV55JcCzy9qt41dD0T5N10J55dW1VfTvJE4LaBaxrKJ5N8E/hb4E1JZoC/G7imIdw9zcHe+wjwF8BvAaPXprivqr47TEkLm+qTmJL8VVW9aOg6NJmSHAfcW1U7kzwSOKaq/u/QdR0KSc7t7/408Djgz4H7dz1eVR8foKzB9deqOJGRjnFV3TFcRXObyp77iBuSbAT+FPjhrsYp/uC+B/gNut7qp4FnAm+pqg8NWtgAklwEfLiqdvZNR9Mdi7hsuKoOqVeM3P8R8JKR7QKm7m+kX1LlncDdwEN9cwETOats2nvuH5iluarqjYe8mAmQ5KtV9awkPwe8Cvg3wGer6pnDVnbo7Xov9mq7oaqePVBJGliSLXTXpfjO0LWMY6p77lX1z4euYcJ4EHG3I5Kk+t5P/3X86IFrOuSSrAfeXFXf77ePA947pR2gO4EfDF3EuKY63JOsoJvXfgbd16tr6T7IWwctbDgeRNztL4ENSX6f7rPxi3RDVdPmGbuCHaCqvpdkWr+9fAu4Jsmn2PP4w0SeCzLtwzJX0x0J/2Df9HrgdVU123zWqTDNBxFHJTkC+JfAmXRXFvsM8IcjY/BTIcnXgBdW1ff67eOBz1XV04et7NBL8o7Z2id1xt20h/ts46r7tE2LJI8A3go8vqouTLIaeEpVXTVwaRpIkn8GXAL8Gd03mNcA66rqg/M+sWFJHllVP1x4z2FN5RmqI+5J8voky/qf1wOHxcGSJfIB4AHgBf32VrrZM1MjyYb+9utJbtz7Z+j6DrWq+hPgn9DNENkBnDutwd6f2X4zcEu//cwkEzt7atp77o8H/gvw/L7pC3Rj7n8zXFXD2XXB39FZIUm+Nk2zZZIsr6ptSZ4w2+PT+NlI8lPA6qr6QH8c5lFVdfvQdR1qSb5Edwb7xpG/j29U1dOGrWx2U31AtT/54JVD1zFBHkjyE3Rfv0nyJEYOHE2DqtrW331TVb199LF+kai37/usdvXjzGuAp9B9szsK+BDdJISpU1V37jWDbGKPwUz1sEySJyb5ZJIdSbYnubI/5X5avYNuRsjJST4MbALeNmxJgzmsFolaQj9H1wH6IUBV3cWe6zFNkzuTvACoJEcn+bf0QzSTaKp77nQzZX6P7gMMsBb4KPC8wSoaUFVdneQrwOl0M0TeXFX3DFzWIXW4LhK1hB6oqkqy69vcI4cuaEC/CPwucBLd8ajPABcNWtE8pn3M/UtV9by92v66qk4fqqahJTkJeAJ7rp3x+eEqOrSSPBo4jsNskail0vdOV9N9k/kt4I3AR6pqaq97cLiY9p77Z5NcDFxBN87888Cn+rm8TNsfcz+m/PPATey5dsbUhDvd8hPf7teW2UOS46ftMwHM0E2DvJdu3P0/MqVr/CdZBfwrYCV7dn4m8rjdtPfcR4/473ojdh0tqaqaqvH3JLfSnZE4VQdRRyW5qqrO7j8bxe7PA0znZ+IrVfWcvdpurKqJXCxrKfUndP0R8HV2d36oqs8NVtQ8pr3n/nbg01V1b5L/ADwH+PWq+srAdQ3lW3SzIaY23Kvq7P521dC1DMljD7P6u6p639BFjGvae+43VtUz+nm8vwm8F/i1vcfhp0WSj9Et87uJPdfO+NeDFXWIJXnOfI9Py3/8HnvYV5J/Snf84TPs+fcxkZ+Jae+575qj+nLg96vqynTXUZ1WG/ufafbeeR4rYCou7lJVP6BbAfG1Q9cyQZ4OvIHuMzB6TGoiPxPT3nO/Cvg/dAeInku3GuJ103RGpqTx9CumPqOqHhi6lnFMe8/9NcBZwH+qqu/3F4eexivcb6iq1yT5OrsPLEN3MLGm9ODZUcAvAf+ob7oG+IOq+vvBitLQvgY8Btg+cB1jmeqeuzqup7KvJH9Id3B5fd/0BmBnVf3CcFVpSEmuobuk3pfZc8zdqZCabP3Zh39bVQ8leTLwk8BfTGNvdbYF06ZtETXtKclPz9buVEgdDj4P/MP+gh2bgM10JzW9btCqhrEzyZOq6n9Dtw4RE7xIlJbepIb4XAx3jUpV/SjJBcD7q+o9SW4YuqiB/CrdGczf6rdXAl5zd4olORf4beAEuuNRu45JHTtoYXOY6lUhtY8keT5dT/1Tfdu0dgC+APwB3ZS3h/r7Xxy0Ig3tPcArq+rRVXVsVR0zqcEO0/uHq9m9he6Sap+oqpv6oYjPDlvSYP6Ebj2VX++3X0t3rd3zBqtIQ7u7qiZ2id+9eUBVmoUHVLW3JL8LPA74c/acLfPxoWqajz13/ViSz7LnPHcAqmoiz8BbYjckOb2q/hogyfOY3jVV1DkW+BHwkpG2AiYy3O2568eSPHdk8+F0F0Z+sKqm7mpMSW6hW+L2jr7p8XRX3XmIKT2xS4cXw13zSvK5qpp1fm/L5jqha5dpPLFrWiV5Wz9z7P3M/s12IhfWc1hGP7brIiW9I+gujPy4gcoZlOGtEbsOom4etIr9ZM9dPzZygQqAB4FvA++uqmsHK0rSAbHnrlGn0F2g4afoQv5/cZj1VqSlkmSG7gI/p9AdkwImd8KBJzFp1HrgqcD7gPf39z84aEXS5Pgw3RDNKuBddN9svzxkQfNxWEY/5txuaW5Jrq+q545eQ3aSJxzYc9eoG5KcvmvDud3SHnatjrotycuTPBtYMWRB87HnLkYu0nEUu+d2F/AE4OaqetqA5UkTIcnZdMehTqYbtjwWeGdVfXLQwubgAVUBnD10AdJh4Hsj15b9GYAkZwxb0tzsuUvSGJJ8paqes1DbpLDnLknz6JfBfgEwk+StIw8dCywbpqqFGe6SNL+jgUfR5eUxI+33Aq8epKIxOCwjSQtIsgz4H1U1sWG+N6dCStICqmoncPyCO04Qh2UkaTw3JNkI/Cnww12NXqxDkg5vxwPfAUbXkvFiHZKkQ8cxd0kaQ5InJ9mU5Bv99jOS/Puh65qL4S5J4/lvwCX0a8xU1Y3A2kErmofhLknjeURVXbdX24ODVDIGw12SxnNPkifRX60syauBbcOWNDcPqErSGJI8EbicbimC7wG3A6+b1OvtOhVSksZTVfXiJI8Ejqiq+5KsGrqouTgsI0nj+RhAVf2wqu7r2/5swHrmZc9dkuaR5CeBU4FHJzl35KFjGblQ9qQx3CVpfk+hu6DNY4BXjLTfB/yLIQoahwdUJWkMSZ5fVV8cuo5xGe6SNIYkM3Q99ZWMjHpU1RuHqmk+DstI0niupLtA9v8Edg5cy4LsuUvSGJJ8taqeNXQd43IqpCSN56okLxu6iHHZc5ekMSS5D3gE8ADd4mGhO7Hp2EELm4Nj7pI0nkcDrwNWVdW7kzweWD5wTXOy5y5JY0jyX4GHgBdV1VOTHAd8pqr+wcClzcqeuySN53lV9ZwkNwBU1feSHD10UXPxgKokjefvkyxj95K/M3Q9+YlkuEvSeN4HfAI4Ick64FrgN4ctaW6OuUvSmPpFxM6kmymzqapuGbikORnuktQgh2UkqUGGuyQ1yHCXpAYZ7pLUoP8PHp637Ou7mWkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking number of records of each label\n",
    "df['label'].value_counts().sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quarterly profits at US media giant TimeWarn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The dollar has hit its highest level against...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The owners of embattled Russian oil giant Yu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>British Airways has blamed high fuel prices ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shares in UK drinks and food firm Allied Dom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0    Quarterly profits at US media giant TimeWarn...      0\n",
       "1    The dollar has hit its highest level against...      0\n",
       "2    The owners of embattled Russian oil giant Yu...      0\n",
       "3    British Airways has blamed high fuel prices ...      0\n",
       "4    Shares in UK drinks and food firm Allied Dom...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode label\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# prepossing text\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "#     text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(clean_text)\n",
    "df['text'] = df['text'].str.replace('\\d+', '')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing text for BERT model\n",
    "def get_split(text1):\n",
    "    l_total = []\n",
    "    l_parcial = []\n",
    "    if len(text1.split())//150 >0:\n",
    "        n = len(text1.split())//150\n",
    "    else: \n",
    "        n = 1\n",
    "    for w in range(n):\n",
    "        if w == 0:\n",
    "            l_parcial = text1.split()[:200]\n",
    "            l_total.append(\" \".join(l_parcial))\n",
    "        else:\n",
    "            l_parcial = text1.split()[w*150:w*150 + 200]\n",
    "            l_total.append(\" \".join(l_parcial))\n",
    "    return l_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_split'] = df['text'].apply(get_split)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting output directory for BERT\n",
    "OUTPUT_DIR = 'bert_news_category'\n",
    "\n",
    "#@markdown Whether or not to clear/delete the directory and create a new one\n",
    "DO_DELETE = True #@param {type:\"boolean\"}\n",
    "\n",
    "if DO_DELETE:\n",
    "    try:\n",
    "        tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "#         tf.compat.v1.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT DATA\n",
    "train, val = train_test_split(df, test_size=0.2, random_state=35)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "label_list = [x for x in np.unique(train.label)]\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.reset_index(drop=True, inplace=True)\n",
    "val.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l = []\n",
    "label_l = []\n",
    "index_l =[]\n",
    "for idx,row in train.iterrows():\n",
    "    for l in row['text_split']:\n",
    "        train_l.append(l)\n",
    "        label_l.append(row['label'])\n",
    "        index_l.append(idx)\n",
    "len(train_l), len(label_l), len(index_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_l = []\n",
    "val_label_l = []\n",
    "val_index_l = []\n",
    "for idx,row in val.iterrows():\n",
    "    for l in row['text_split']:\n",
    "        val_l.append(l)\n",
    "        val_label_l.append(row['label'])\n",
    "        val_index_l.append(idx)\n",
    "len(val_l), len(val_label_l), len(val_index_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Getting train and validation set as dataframe¶\n",
    "train_df = pd.DataFrame({DATA_COLUMN:train_l, LABEL_COLUMN:label_l})\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame({DATA_COLUMN:val_l, LABEL_COLUMN:val_label_l})\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fine tuning the BERT model\n",
    "The BERT model can be applied for any kind of classification task by fine-tuning it.\n",
    "\n",
    "1. Preparing the input data, i.e create InputExample using the BERT’s constructor.\n",
    "'''\n",
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "val_InputExamples = val.apply(lambda x: run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "train_InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "BERT_MODEL_HUB2 = os.sep.join([\"..\",\"..\",\"tensor_hub_model_bert\"])\n",
    "print(BERT_MODEL_HUB2)\n",
    "#\"D:\\git_r\\bert_parc1\\sean_bert_uncased_L-12_H-768_A-12_1z\"\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    #bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    # load from local\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB2)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "#     with tf.compat.v1.Session() as sess:\n",
    "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is what the tokenised sample of the first training set observation looks like\n",
    "print(tokenizer.tokenize(train_InputExamples.iloc[0].text_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. Converting the train and validation features to InputFeatures that BERT understands.¶\n",
    "'''\n",
    "MAX_SEQ_LENGTH = 200\n",
    "\n",
    "train_features = run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "val_features = run_classifier.convert_examples_to_features(val_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Example on first observation in the training set\n",
    "print(\"Sentence : \", train_InputExamples.iloc[0].text_a)\n",
    "print(\"-\"*30)\n",
    "print(\"Tokens : \", tokenizer.tokenize(train_InputExamples.iloc[0].text_a))\n",
    "print(\"-\"*30)\n",
    "print(\"Input IDs : \", train_features[0].input_ids)\n",
    "print(\"-\"*30)\n",
    "print(\"Input Masks : \", train_features[0].input_mask)\n",
    "print(\"-\"*30)\n",
    "print(\"Segment IDs : \", train_features[0].segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating prediction model¶\n",
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "  \n",
    "    bert_module = hub.Module(\n",
    "        BERT_MODEL_HUB2,\n",
    "        trainable=True)\n",
    "    bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    # Use \"sequence_outputs\" for token-level output.\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "    # with tf.Session() as sess:\n",
    "    output_layer1 = bert_outputs[\"pooled_output\"]\n",
    "    # output_layer1 = 999\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    # Create our own layer to tune for politeness data.\n",
    "    output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "\n",
    "        # Dropout helps prevent overfitting\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.8)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Convert labels into one-hot encoding\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs, output_layer1)\n",
    "\n",
    "        # If we're train/eval, compute loss between predicted and actual label\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        \n",
    "        return (loss, predicted_labels, log_probs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "    \n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "\n",
    "        # TRAIN and EVAL\n",
    "        if not is_predicting:\n",
    "\n",
    "            (loss, predicted_labels, log_probs) = create_model(\n",
    "            is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "            train_op = optimization.create_optimizer(\n",
    "              loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "            # Calculate evaluation metrics. \n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                true_pos = tf.metrics.true_positives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                true_neg = tf.metrics.true_negatives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)   \n",
    "                false_pos = tf.metrics.false_positives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)  \n",
    "                false_neg = tf.metrics.false_negatives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "\n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                    \"true_positives\": true_pos,\n",
    "                    \"true_negatives\": true_neg,\n",
    "                    \"false_positives\": false_pos,\n",
    "                    \"false_negatives\": false_neg,\n",
    "                    }\n",
    "\n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                  loss=loss,\n",
    "                  train_op=train_op)\n",
    "            else:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                    loss=loss,\n",
    "                    eval_metric_ops=eval_metrics)\n",
    "        else:\n",
    "            (predicted_labels, log_probs, output_layer) = create_model(\n",
    "            is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "            predictions = {\n",
    "              'probabilities': log_probs,\n",
    "              'labels': predicted_labels,\n",
    "              'pooled_output': output_layer\n",
    "            }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Return the actual model function in the closure\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.1\n",
    "NUM_TRAIN_EPOCHS = 1.0\n",
    "# Warmup is a period of time where the learning rate is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 300\n",
    "SAVE_SUMMARY_STEPS = 100\n",
    "\n",
    "# Compute train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "# Specify output directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
    "\n",
    "\n",
    "# gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "# session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps, len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##　Initializing the model and the estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)\n",
    "\n",
    "# Create an input function for validating. drop_remainder = True for using TPUs.\n",
    "val_input_fn = run_classifier.input_fn_builder(\n",
    "    features=val_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "# Training\n",
    "#\n",
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation\n",
    "#Evaluating the model with Validation set\n",
    "estimator.evaluate(input_fn=val_input_fn, steps=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
